{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:04:31.432939Z","iopub.status.busy":"2024-03-11T16:04:31.431515Z","iopub.status.idle":"2024-03-11T16:04:47.196479Z","shell.execute_reply":"2024-03-11T16:04:47.195444Z","shell.execute_reply.started":"2024-03-11T16:04:31.432876Z"}},"source":["# Combine text and image feature vectors and use different clustering algorithm\n","**This notebook runs on Kaggle.**\n","In this notebook we combine image features and text features to do clustering with best performing algorithms. We use both Bag of Words approach and Sentence Transformers\n","\n","Needed for this notebook:\n","* Pre-processed CSV file using [data_exploration_and_cleaning.ipynb](data_exploration_and_cleaning.ipynb) with name in English and German (both clean) \n","* Image features dictionary in a pickle file generated in [finetuning-resnet50.ipynb](finetuning-resnet50.ipynb) or [finetuning-vgg16.ipynb](finetuning-vgg16.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:04:31.432939Z","iopub.status.busy":"2024-03-11T16:04:31.431515Z","iopub.status.idle":"2024-03-11T16:04:47.196479Z","shell.execute_reply":"2024-03-11T16:04:47.195444Z","shell.execute_reply.started":"2024-03-11T16:04:31.432876Z"},"trusted":true},"outputs":[],"source":["# Line below is in case you are executing in collab or kaggle and need to install sentence transformers\n","!pip install sentence_transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:10:52.573472Z","iopub.status.busy":"2024-03-11T16:10:52.573058Z","iopub.status.idle":"2024-03-11T16:11:11.852767Z","shell.execute_reply":"2024-03-11T16:11:11.851715Z","shell.execute_reply.started":"2024-03-11T16:10:52.573440Z"},"trusted":true},"outputs":[],"source":["# import os\n","import pickle\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","# For embedding text\n","from sentence_transformers import SentenceTransformer\n","from sklearn.preprocessing import normalize\n","from sklearn.cluster import DBSCAN, KMeans\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# models\n","import tensorflow as tf\n","print('TF Version:', tf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["## Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:13:06.105994Z","iopub.status.busy":"2024-03-11T16:13:06.104989Z","iopub.status.idle":"2024-03-11T16:13:06.112417Z","shell.execute_reply":"2024-03-11T16:13:06.111438Z","shell.execute_reply.started":"2024-03-11T16:13:06.105960Z"},"trusted":true},"outputs":[],"source":["def dump_to_pickle_file(parts_feature, dump_path):\n","    '''\n","    Dump a dictionary to a pickle file.\n","\n","    Parameters:\n","    parts_feature (dict): Dictionary to be saved.\n","    dump_path (str): Path to the pickle file.\n","    '''\n","    with open(dump_path, 'wb') as file:\n","        pickle.dump(parts_feature, file)\n","\n","def load_from_pickle_file(pickle_path):\n","    '''\n","    Load a dictionary from a pickle file.\n","\n","    Parameters:\n","    pickle_path (str): Path to the pickle file.\n","\n","    Returns:\n","    dict: Loaded dictionary.\n","    '''\n","    with open(pickle_path, 'rb') as file: \n","        return pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:13:08.827983Z","iopub.status.busy":"2024-03-11T16:13:08.827250Z","iopub.status.idle":"2024-03-11T16:13:08.835453Z","shell.execute_reply":"2024-03-11T16:13:08.834421Z","shell.execute_reply.started":"2024-03-11T16:13:08.827952Z"},"trusted":true},"outputs":[],"source":["# This set of functions helps in creating BOW\n","import unicodedata\n","import re\n","\n","def remove_accented_chars(text):\n","    \"\"\"\n","    Remove accented characters from a string.\n","\n","    Args:\n","        text (str): Input text containing accented characters.\n","\n","    Returns:\n","        str: Text with accented characters replaced with their ASCII equivalents.\n","    \"\"\"\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return text\n","\n","def pre_process_corpus(docs):\n","    \"\"\"\n","    Pre-process a list of documents for text analysis.\n","\n","    Args:\n","        docs (list of str): List of documents to be pre-processed.\n","\n","    Returns:\n","        list of str: List of pre-processed documents.\n","    \"\"\"\n","    norm_docs = []\n","    for doc in docs:\n","        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n","        doc = doc.lower()\n","        doc = remove_accented_chars(doc)\n","        # lower case and remove special characters\\whitespaces\n","        doc = re.sub(r'[^a-zA-Z0-9\\s]', ' ', doc, flags=re.I|re.A)\n","        doc = re.sub(' +', ' ', doc)\n","        doc = doc.strip()\n","        norm_docs.append(doc)\n","    return norm_docs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:13:11.088030Z","iopub.status.busy":"2024-03-11T16:13:11.087293Z","iopub.status.idle":"2024-03-11T16:13:11.097962Z","shell.execute_reply":"2024-03-11T16:13:11.096874Z","shell.execute_reply.started":"2024-03-11T16:13:11.087997Z"},"trusted":true},"outputs":[],"source":["# This helper function helps evaluate\n","def print_evaluation(target, labels, itemnumber, remove_noise = False):\n","    \"\"\"\n","    Print evaluation metrics for clustering results.\n","\n","    Args:\n","        target (list): List of true target labels.\n","        labels (list): List of predicted cluster labels.\n","        itemnumber (list): List of item numbers corresponding to the samples.\n","        remove_noise (bool, optional): Whether to remove noise clusters. Defaults to False.\n","    \"\"\"\n","    # create df with inputs\n","    d = {\"target\" : target, \"cluster\":labels,\"ItemNumber\":itemnumber}\n","    cluster_labels = pd.DataFrame(d)\n","    if remove_noise:\n","        cluster_labels = cluster_labels[~cluster_labels['cluster'].isin([-1])]\n","    cluster_nums=cluster_labels.cluster.unique()\n","    # We create a ccluster map assignning each cluster to most frequent target_subfamily class in it\n","    cluster_map = {}\n","    for cluster in cluster_nums:\n","        cluster_map[cluster] = cluster_labels[cluster_labels.cluster.isin([cluster])].target.value_counts().index[0]\n","    #  We print results\n","    print(\"-------------------------------------\")\n","    cluster_labels[\"predicted_target\"] = cluster_labels.cluster.map(cluster_map) \n","    cluster_labels[\"correct\"] = cluster_labels.apply(lambda x: 1 if x[\"target\"]== x[\"predicted_target\"] else 0, axis =1)\n","    print(\"Number of samples\",cluster_labels.correct.count())\n","    print(\"Number of clusters:\",cluster_labels.cluster.nunique())\n","    print(cluster_labels.correct.value_counts())\n","    print(\"Percentage correct:\",cluster_labels[cluster_labels[\"correct\"] ==1].correct.count()/cluster_labels.correct.count())\n","    print(\"------------------------------------------\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data and image features loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:17:48.487086Z","iopub.status.busy":"2024-03-11T16:17:48.486644Z","iopub.status.idle":"2024-03-11T16:17:48.631942Z","shell.execute_reply":"2024-03-11T16:17:48.630755Z","shell.execute_reply.started":"2024-03-11T16:17:48.487055Z"},"trusted":true},"outputs":[],"source":["# import cleaned csv file\n","PROCESSED_DATA_CSV_PATHFILE=\"/kaggle/input/syrus-data/SyrusMasterDataAnonymisedProc.csv\"\n","df = pd.read_csv(PROCESSED_DATA_CSV_PATHFILE)\n","df = df[df['EnglishItemNameClean'].notna()]\n","df = df[df['hasImage']==1]\n","df['FileName'] = df['ItemNumber'].apply(lambda x: x+\".jpeg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:09:37.918709Z","iopub.status.busy":"2024-03-11T17:09:37.918271Z","iopub.status.idle":"2024-03-11T17:09:39.171961Z","shell.execute_reply":"2024-03-11T17:09:39.171043Z","shell.execute_reply.started":"2024-03-11T17:09:37.918679Z"},"trusted":true},"outputs":[],"source":["# load image feature dictionary\n","IMAGE_FEATURES_PATH = \"/kaggle/input/syrus-data/features_resnet50_finetuned_segmented_final_full_data.pkl\"\n","image_features = load_from_pickle_file(IMAGE_FEATURES_PATH)\n","image_feature_df = pd.DataFrame(list(image_features.items()), columns=['ItemNumber', 'ImageFeature'])\n","# Strip \".jpeg\"\n","image_feature_df[\"ItemNumber\"] = image_feature_df[\"ItemNumber\"].apply(lambda x: x.strip('.jpeg'))\n","# merge image features into the CSV with other data\n","df_merged = pd.merge(df, image_feature_df, on='ItemNumber', how='inner')\n","# create a matrix with features\n","ImFeatureMatrix= np.vstack(df_merged['ImageFeature'])"]},{"cell_type":"markdown","metadata":{},"source":["## Feature extraction from English item names using Sentence Transformer or BOW and clustering using DBSCAN or k-means"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T16:27:43.362726Z","iopub.status.busy":"2024-03-11T16:27:43.361977Z","iopub.status.idle":"2024-03-11T16:28:02.194961Z","shell.execute_reply":"2024-03-11T16:28:02.194009Z","shell.execute_reply.started":"2024-03-11T16:27:43.362694Z"},"trusted":true},"outputs":[],"source":["# load embedder sentence transformer\n","EMBEDDER = SentenceTransformer('sentence-t5-large')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:09:45.695302Z","iopub.status.busy":"2024-03-11T17:09:45.694827Z","iopub.status.idle":"2024-03-11T17:09:53.287661Z","shell.execute_reply":"2024-03-11T17:09:53.286788Z","shell.execute_reply.started":"2024-03-11T17:09:45.695273Z"},"trusted":true},"outputs":[],"source":["# create embeddings\n","embeddings = EMBEDDER.encode(pre_process_corpus(df_merged['EnglishItemNameClean']))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:09:56.625628Z","iopub.status.busy":"2024-03-11T17:09:56.624379Z","iopub.status.idle":"2024-03-11T17:09:56.750790Z","shell.execute_reply":"2024-03-11T17:09:56.749787Z","shell.execute_reply.started":"2024-03-11T17:09:56.625586Z"},"trusted":true},"outputs":[],"source":["# add image features to text embeddings by vertically stacking all together. Each row is one spare part (one data point)\n","full_features_unnormalized = np.hstack((embeddings,ImFeatureMatrix))\n","# normalize so so that all features have same importance\n","full_features = normalize(full_features_unnormalized, axis=0, norm='l1')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:09:59.448854Z","iopub.status.busy":"2024-03-11T17:09:59.448471Z","iopub.status.idle":"2024-03-11T17:10:01.965291Z","shell.execute_reply":"2024-03-11T17:10:01.964237Z","shell.execute_reply.started":"2024-03-11T17:09:59.448826Z"},"trusted":true},"outputs":[],"source":["# perform density based clustering on combined image features and sentence transormer embedding\n","dbscanModel = DBSCAN(eps = 0.1, min_samples = 5, n_jobs = -1, metric= \"cosine\")\n","labels = dbscanModel.fit_predict(full_features)\n","print_evaluation(df_merged.target, labels, df_merged.ItemNumber,  remove_noise = True )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:10:05.098276Z","iopub.status.busy":"2024-03-11T17:10:05.097847Z","iopub.status.idle":"2024-03-11T17:17:02.850584Z","shell.execute_reply":"2024-03-11T17:17:02.849453Z","shell.execute_reply.started":"2024-03-11T17:10:05.098243Z"},"trusted":true},"outputs":[],"source":["# perform kmeans clustering on combined image features and sentence transormer embedding\n","true_k = 500\n","model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300)\n","labels = model.fit_predict(full_features)\n","print_evaluation(df_merged.target, labels, df_merged.ItemNumber)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:17:02.852683Z","iopub.status.busy":"2024-03-11T17:17:02.852265Z","iopub.status.idle":"2024-03-11T17:17:02.998076Z","shell.execute_reply":"2024-03-11T17:17:02.997225Z","shell.execute_reply.started":"2024-03-11T17:17:02.852655Z"},"trusted":true},"outputs":[],"source":["# create BOW\n","cv = CountVectorizer()\n","cv_features = cv.fit_transform(pre_process_corpus(df_merged['EnglishItemNameClean']))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:22:56.815699Z","iopub.status.busy":"2024-03-11T17:22:56.815123Z","iopub.status.idle":"2024-03-11T17:22:57.112682Z","shell.execute_reply":"2024-03-11T17:22:57.111470Z","shell.execute_reply.started":"2024-03-11T17:22:56.815660Z"},"trusted":true},"outputs":[],"source":["# add image features to text BOW by vertically stacking all together. Each row is one spare part (one data point)\n","full_features_unnormalized = np.hstack((np.asarray(cv_features.todense()),ImFeatureMatrix))\n","# normalize so so that all features have same importance\n","full_features = normalize(full_features_unnormalized, axis=0, norm='l1')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:17:03.298791Z","iopub.status.busy":"2024-03-11T17:17:03.298479Z","iopub.status.idle":"2024-03-11T17:17:08.468518Z","shell.execute_reply":"2024-03-11T17:17:08.467435Z","shell.execute_reply.started":"2024-03-11T17:17:03.298758Z"},"trusted":true},"outputs":[],"source":["# perform density based clustering with combined image features and BOW and evaluate removing noise points\n","dbscanModel = DBSCAN(eps = 0.1, min_samples = 5, n_jobs = -1, metric= \"cosine\")\n","labels = dbscanModel.fit_predict(full_features)\n","print_evaluation(df_merged.target, labels, df_merged.ItemNumber,  remove_noise = True )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T17:17:08.470727Z","iopub.status.busy":"2024-03-11T17:17:08.470213Z","iopub.status.idle":"2024-03-11T17:19:25.754573Z","shell.execute_reply":"2024-03-11T17:19:25.753445Z","shell.execute_reply.started":"2024-03-11T17:17:08.470688Z"},"trusted":true},"outputs":[],"source":["# perform kmeans clustering with combined image features and BOW and evaluate removing noise points\n","true_k = 500\n","model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300)\n","labels = model.fit_predict(full_features)\n","print_evaluation(df_merged.target, labels, df_merged.ItemNumber)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4412417,"isSourceIdPinned":true,"sourceId":7670050,"sourceType":"datasetVersion"},{"sourceId":162041479,"sourceType":"kernelVersion"},{"sourceId":162620360,"sourceType":"kernelVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
