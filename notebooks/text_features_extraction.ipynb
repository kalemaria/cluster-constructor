{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317e0af4-d8f7-4ac1-94f6-eccf80c7443a",
   "metadata": {},
   "source": [
    "# Extracting Feature Vectors from Text\n",
    "**This notebook runs locally.**\n",
    "\n",
    "It extracts feature vectors from the text file name. It uses both Bag of Words approach and Sentence Transformer.\n",
    "\n",
    "Needed for this notebook:\n",
    "* Pre-processed CSV file using [data_exploration_and_cleaning.ipynb](data_exploration_and_cleaning.ipynb) with name in English and German (both clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e43fd251-43c6-4059-84c5-54aea15e57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a564663-6054-4e3b-9d43-e7bb2c4c4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv file\n",
    "PROCESSED_DATA_CSV_PATHFILE=\"../../data/processed/SyrusMasterDataAnonymisedProc.csv\"\n",
    "df = pd.read_csv(PROCESSED_DATA_CSV_PATHFILE)\n",
    "df = df[df['GermanItemNameClean'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed365b",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4be62c-5259-4a15-8986-0678155a0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"\n",
    "    Remove accented characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text containing accented characters.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with accented characters replaced with their ASCII equivalents.\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    \"\"\"\n",
    "    Pre-process a list of documents for text analysis.\n",
    "\n",
    "    Args:\n",
    "        docs (list of str): List of documents to be pre-processed.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of pre-processed documents.\n",
    "    \"\"\"\n",
    "    norm_docs = []\n",
    "    for doc in docs:\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        doc = remove_accented_chars(doc)\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', ' ', doc, flags=re.I|re.A)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "        norm_docs.append(doc)\n",
    "    return norm_docs\n",
    "\n",
    "def print_evaluation(target, labels, itemnumber, remove_noise = False ):\n",
    "    \"\"\"\n",
    "    Print evaluation metrics for clustering results.\n",
    "\n",
    "    Args:\n",
    "        target (list): List of true target labels.\n",
    "        labels (list): List of predicted cluster labels.\n",
    "        itemnumber (list): List of item numbers corresponding to the samples.\n",
    "        remove_noise (bool, optional): Whether to remove noise clusters. Defaults to False.\n",
    "    \"\"\"\n",
    "    # create df with inputs\n",
    "    d = {\"target\" : target, \"cluster\":labels,\"ItemNumber\":itemnumber  }\n",
    "    cluster_labels = pd.DataFrame(d)\n",
    "    if remove_noise:\n",
    "        cluster_labels = cluster_labels[~cluster_labels['cluster'].isin([-1])]\n",
    "    cluster_nums=cluster_labels.cluster.unique()\n",
    "    # create a cluster map assignning each cluster to most frequent target_subfamily class in it\n",
    "    cluster_map = {}\n",
    "    for cluster in cluster_nums:\n",
    "        cluster_map[cluster] = cluster_labels[cluster_labels.cluster.isin([cluster])].target.value_counts().index[0]\n",
    "    # print results\n",
    "    print(\"-------------------------------------\")\n",
    "    cluster_labels[\"predicted_target\"] = cluster_labels.cluster.map(cluster_map) \n",
    "    cluster_labels[\"correct\"] = cluster_labels.apply(lambda x: 1 if x[\"target\"]== x[\"predicted_target\"] else 0, axis =1)\n",
    "    print(\"Number of samples\",cluster_labels.correct.count())\n",
    "    print(\"Number of clusters:\",cluster_labels.cluster.nunique())\n",
    "    print(cluster_labels.correct.value_counts())\n",
    "    print(\"Percentage correct:\",cluster_labels[cluster_labels[\"correct\"] ==1].correct.count()/cluster_labels.correct.count())\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae8a5f0-28ee-46a5-89d5-a073fd2b7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate clean text\n",
    "pre_proc_text = pre_process_corpus(df.EnglishItemName.values)\n",
    "# create Bag of Words (BOW)\n",
    "cv = CountVectorizer()\n",
    "cv_features = cv.fit_transform(pre_proc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417cebad-4e77-437d-8781-5b7d0c751510",
   "metadata": {},
   "source": [
    "## K-means clustering using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "938b6ddb-1363-456b-9d96-67414a53ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\miniconda3\\envs\\syrus\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Number of samples 13369\n",
      "Number of clusters: 500\n",
      "correct\n",
      "1    9186\n",
      "0    4183\n",
      "Name: count, dtype: int64\n",
      "Percentage correct: 0.6871119754656294\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# do a Kmeans clustering only on BOW features and print resutls using helper function\n",
    "true_k = 500\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300)\n",
    "labels = model.fit_predict(cv_features)\n",
    "print_evaluation(df.target, labels, df.ItemNumber )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b21d0b-1782-4753-a09b-038a817caf76",
   "metadata": {},
   "source": [
    "## Clustering using Sentence Transformer instead of BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6db909-8635-4f25-bd81-bcf08bea0083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\miniconda3\\envs\\syrus\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import and load sentence transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('sentence-t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c01f27ff-876b-419e-9974-bcaf30c61def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use loaded embedder to extract feature vectors from cleaned text\n",
    "embeddings = embedder.encode(pre_proc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8a4f841-5ba3-4aa9-98f0-de9f35618917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\miniconda3\\envs\\syrus\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Number of samples 13369\n",
      "Number of clusters: 500\n",
      "correct\n",
      "1    9133\n",
      "0    4236\n",
      "Name: count, dtype: int64\n",
      "Percentage correct: 0.6831475802229038\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "true_k = 500\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300)\n",
    "labels = model.fit_predict(embeddings)\n",
    "print_evaluation(df.target, labels, df.ItemNumber )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637f1e5-8cda-4c5c-a6b2-634b9be60d93",
   "metadata": {},
   "source": [
    "## Clustering using DBSCAN algorithm and removing noise labelled data points for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ba3466-a61e-438c-8286-38112b32aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Number of samples 6325\n",
      "Number of clusters: 196\n",
      "correct\n",
      "1    4386\n",
      "0    1939\n",
      "Name: count, dtype: int64\n",
      "Percentage correct: 0.6934387351778656\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# First with BOW\n",
    "dbscanModel = DBSCAN(eps = 0.1, min_samples = 5, n_jobs = -1, metric= \"cosine\")\n",
    "labels = dbscanModel.fit_predict(cv_features)\n",
    "print_evaluation(df.target, labels, df.ItemNumber,  remove_noise = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56dce942-46b1-4668-a064-6dfa755b53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Number of samples 12089\n",
      "Number of clusters: 68\n",
      "correct\n",
      "0    9755\n",
      "1    2334\n",
      "Name: count, dtype: int64\n",
      "Percentage correct: 0.19306807841839688\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# second with transformers\n",
    "dbscanModel = DBSCAN(eps = 0.1, min_samples = 5, n_jobs = -1, metric= \"cosine\")\n",
    "labels = dbscanModel.fit_predict(embeddings)\n",
    "print_evaluation(df.target, labels, df.ItemNumber,  remove_noise = True )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syrus",
   "language": "python",
   "name": "syrus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
